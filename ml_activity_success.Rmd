---
title: 'Machine Learning: Identify the Successful Completion of Weight-Lifting Exercises'
author: "Roberto Rivera"
date: "01/28/2017"
footer: "What's up"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: united
    highlight: zenburn
    include:
      after_body: footer.html
  pandoc_args:
    - +RTS
    - -K64m
    - -RTS
  pdf_document: default
---

<style type="text/css">
blockquote {
  font-size: 13px;
  font-style: italic
}
blockquote p {
  display: inline;
}
code.r{ /* Code block */
  font-size: 11px;
}
pre { /* Code block */
  font-size: 11px
}
</style>

## Background

With devices like the Jawbone Up, Nike FuelBand and Fitbit, it is feasible to inexpensively collect and analyze a large quantity of Human Activity Recognition (HAR) data. Although users of these devices tend to quantify how much they participate in an activity, they rarely consider "how well" they perform the activity. Using HAR data from [Groupware@LES][1] we aim to leverage information regarding weight-lifting exercises to predict how well an exercise was performed by the users of HAR devices. Six participants in the Groupware study were asked to perform dumbbell exercises with correct form (Class A) and with the most common mistakes: throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

## Objective

Using this HAR class data, we plan to leverage machine learning algorithms to develop a model that can distinguish between participants that have exercised correctly versus those that didn't, and what their mistakes were. We will then use this model to assess how accurately we can classify the exercises performed by other individuals solely based on their HAR data.

> <u>Technical Notes</u>: This post is statically generated with [RStudio `r getRversion()`][21] using [RMarkdown `r packageVersion("rmarkdown")`][20] for [reproducibility][16]. The complete codebase is available on my [github page][17]. Many code blocks are cached for performant iterative analysis. When updating code blocks, pay special attention to subsequent cached code blocks that should be updated as a result. To learn more, have a peek at the [RStudio][15]/[RMarkdown][18] documentation and [cheatsheets][14].

## Data Processing

### Getting the Data

Download the training and test data, if not already in the project data folder.

```{r get_data, echo=TRUE, eval=TRUE, collapse=TRUE}
setwd("/home/rstudio/code")

trnFile = "data/pml-har-trn.csv"
tstFile = "data/pml-har-tst.csv"
if (!file.exists(trnFile)) {
    trnfileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(trnfileUrl, destfile = trnFile, method = "curl")
}
if (!file.exists(tstFile)) {
    tstfileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(tstfileUrl, destfile = tstFile, method = "curl")
}
```

### Exploring and Preparing the Data

The goal of data prep is to identify the variables that explain as much of the class variance possible. In other words, we need to find the fewest quantity of variables that can explain everything that is going on. To improve model performance and simplify our analysis we discard those variables not related to exercise performance. 

```{r prep_data, echo=TRUE, eval=TRUE, collapse=TRUE}
# Load libraries
library(caret)
library(knitr)
library(corrplot)
library(doMC)
library(ggplot2)
library(pROC)

# Define number of parallel processes
registerDoMC(cores = 8)

# Load the training file
trn.dat = read.csv(trnFile, na.strings=c("NA","NaN", "", " "), stringsAsFactors=FALSE); rm("trnFile")

# Quick exploratory analysis
dim.init <- dim(trn.dat); dim.init
str(trn.dat)

## Removing unnecessary covariates
# Columns not related to exercise performance.
col.nrel <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", 
              "cvtd_timestamp", "new_window", "num_window")
# Columns with near zero variance
nsv <- nearZeroVar(trn.dat, saveMetrics = TRUE)
col.nzvs <- rownames(nsv[nsv$nzv == TRUE, ])
# Update the training set with the remaining fields
col.drops <- c(col.nrel, col.nzvs)
trn.dat.all <- trn.dat[,!(names(trn.dat) %in% col.drops)]

# Identify fields with large quantity of missing values.
col.names = colnames(trn.dat.all)
cnt.min = sum(complete.cases(trn.dat.all))
trn.cnt.df <- data.frame(Field=character(), FieldCnt=integer(), stringsAsFactors=FALSE)
# Create data-frame with field names and counts.
for (fld in 1:length(col.names)) {
    trn.cnt.df[fld,] <- c(col.names[fld], sum(!is.na(trn.dat.all[[col.names[fld]]])))
}
# Filter out the fields with high quantity of missing values.
col.keep <- as.vector(subset(trn.cnt.df, FieldCnt < cnt.min)$Field)
# Update the training set with the remaining fields
trn.dat.all <- trn.dat.all[col.keep]

## Cleanup and default model settings
# Set the classe variable as a factor.
trn.dat.all$classe <- as.factor(trn.dat.all$classe)
dim.finl <- dim(trn.dat.all); dim.finl

# house-cleaning
rm("cnt.min", "col.drops", "col.keep", "col.names", "col.nrel", "col.nzvs", "fld", "trn.cnt.df", "nsv","trn.dat")

# Cutoff parameters
corrRt = 0.80 # Correlation Cutoff 
dSplit = 0.60 # Training Cutoff

## Set model parameters for model tuning and training
cntFld = 10    # Number of cross-validation folds
cntRpt = 8    # Increase the Repeat count for improved cross-validation accuracy
cntTun = 5    # Parameter for tuning accuracy vs resource consumption
```

Our exploratory analysis shows that the file is just shy of twenty thousand records and that some of the variables have near-zero-variance values or have missing values. After discarding them, our variable count drops from `r dim.init[2]` down to `r dim.finl[2]`. The class variables in the provided data is relatively balanced, which simplifies this analysis somewhat.

```{r prep_summ, echo=TRUE, eval=TRUE, collapse=TRUE}
kable(t(table(trn.dat.all$classe)), caption = "Class Variable Frequency:")
```

### Handling Multicollinearity {.tabset .tabset-fade}

[Multicollinearity][6] occurs when model variables are correlated to your response variable as well as each other. This overinflates the standard errors which makes some variables seem statistically insignificant when they should be significant. We have a few options for dealing with this issue. 

One option is to reduce the number of predictors to a smaller set of uncorrelated components using [Partial Least Squares Regression (PLS)][9] or [Principal Components Analysis (PCA)][10]. Another option is to simply remove the variables from the model. Removing predictors may introduce some bias (difference between expected prediction and the truth), but may also reduce the prediction variance (increase accuracy). The goal is to optimize the trade-off between bias and variance. Both options reduce our variable count and speeds our model calculation.

The goal of PCA is to explain the maximum amount of variance with the fewest number of principal components. PCA creates these components by transforming the variables into a smaller sub-space of variables (dimensionality reduction) that are uncorrelated with each other. A challenge with the components is that it can be difficult to explain what is driving model performance.

We are going to approach this three different ways to get a sense of effort and impact:

1) remove the multicollinear predictors, 
2) apply PCA to the multicollinear predictors, and 
3) apply PCA to all the predictors. 

Using the Caret R package, we apply a [Box-Cox Transformation][11] to correct for skewness, center and scale each variable and then apply PCA in one call to pre-process the variables. In addition, we defined a stepwise [Variable Inflaction Factor (VIF)][12] function to identify multicollinear variables within our specified threshold. A value greater than 10 is considered to be highly collinear. The threshold may be set to the specific tolerance required for your analysis. 

```{r setup_vif, echo=TRUE, eval=TRUE, fig.height=8, fig.width=8, cache=TRUE, collapse=TRUE}
# compile stepwise VIF selection function for reducing collinearity among explanatory variables
vif_func<-function(in_frame, thresh=10, trace=T, ...){
  require(fmsb)
  
  if(class(in_frame) != 'data.frame') in_frame<-data.frame(in_frame)
  
  #get initial vif value for all comparisons of variables
  vif_init<-NULL
  var_names <- names(in_frame)
  for(val in var_names){
      regressors <- var_names[-which(var_names == val)]
      form <- paste(regressors, collapse = '+')
      form_in <- formula(paste(val, '~', form))
      vif_init<-rbind(vif_init, c(val, VIF(lm(form_in, data = in_frame, ...))))
  }
  vif_max<-max(as.numeric(vif_init[,2]))

  if(vif_max < thresh){
    if(trace==T){ #print output of each iteration
        prmatrix(vif_init,collab=c('var','vif'), rowlab=rep('', nrow(vif_init)), quote=F)
        cat('\n')
        cat(paste('All variables have VIF < ', thresh,', max VIF ',round(vif_max,2), sep=''),'\n\n')
    }
    return(var_names)
  }
  else {
    in_dat<-in_frame

    #backwards selection of explanatory variables, stops when all VIF values are below 'thresh'
    while(vif_max >= thresh) {
      
      vif_vals<-NULL
      var_names <- names(in_dat)
        
      for(val in var_names){
        regressors <- var_names[-which(var_names == val)]
        form <- paste(regressors, collapse = '+')
        form_in <- formula(paste(val, '~', form))
        vif_add<-VIF(lm(form_in, data = in_dat, ...))
        vif_vals<-rbind(vif_vals,c(val,vif_add))
      }
      max_row<-which(vif_vals[,2] == max(as.numeric(vif_vals[,2])))[1]

      vif_max<-as.numeric(vif_vals[max_row,2])

      if(vif_max<thresh) break
      
      if(trace==T){ #print output of each iteration
        prmatrix(vif_vals,collab=c('var','vif'),rowlab=rep('',nrow(vif_vals)),quote=F)
        cat('\n')
        cat('removed: ',vif_vals[max_row,1],vif_max,'\n\n')
        flush.console()
      }

      in_dat<-in_dat[,!names(in_dat) %in% vif_vals[max_row,1]]
    }
    return(names(in_dat))
  }
}
```

**Select the tabs below to compare multicollinearity across the different approaches versus the baseline**

#### Baseline Correlation Matrix

This plot is difficult to read, but the key take-away is that the baseline variables are *<u>unusable for modelling</u>* due to significant multicollinearity. This is emphasized by the darker colored cross-sections. The summary below the plot highlights the overall multicollinearity.

```{r init_collinearity, echo=TRUE, eval=TRUE, fig.height=6.5, fig.width=6.5, collapse=TRUE}
# Build a correlation matrix on the initial variables, excluding the "classe" variable
corrMatx <- cor(trn.dat.all[, -dim(trn.dat.all)[2]])
corrSumm <- summary(corrMatx[upper.tri(corrMatx)])[1:6]
corrTabl <- t(as.data.frame(as.vector(corrSumm)))

colnames(corrTabl) <- attr(corrSumm, 'names') # c("Minimum", "1st Quartile", "Median", "Mean", "3rd Quartile", "Maximum") #
rownames(corrTabl) <- "Baseline Summary"

# plot the correlation matrix
corrplot(corrMatx, 
         order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.75, tl.col = "black")

# print the correlation summary
kable(dim(trn.dat.all)[2], col.names = c("Variables"), format='html')
kable(corrTabl, digits = 4, align = 'r', format='markdown')
```

#### Drop Correlation Matrix

Using the stepwise VIF function, we dropped variables with collinearity threshold greater than <u>**two**</u>. Notice the fainter colors compared to the initial matrix. Although we significantly improved our collinearity, we may loose some of the predictive variance in the dropped variables.

```{r setup_collinearity_drp, echo=TRUE, eval=TRUE, fig.height=8, fig.width=8, cache=TRUE, collapse=TRUE}
# use stepwise VIF remove the highly correlated values from the training variable list
var.keep <- vif_func(in_frame=trn.dat.all[, -dim(trn.dat.all)[2]], thresh=2, trace=F)

trn.dat.drp  <- trn.dat.all[,c(var.keep, "classe")]
```

```{r handle_collinearity_drp, echo=TRUE, eval=TRUE, fig.height=6, fig.width=6, collapse=TRUE}
# Build a correlation matrix on the initial variables, excluding the "classe" variable
corrMatx <- cor(trn.dat.drp[, -dim(trn.dat.drp)[2]])
corrSumm <- summary(corrMatx[upper.tri(corrMatx)])[1:6]
corrTabl <- t(as.data.frame(as.vector(corrSumm)))

colnames(corrTabl) <- attr(corrSumm, 'names')
rownames(corrTabl) <- "Drop Summary"

# plot the correlation matrix
corrplot(corrMatx, 
         order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.75, tl.col = "black")

# print the correlation summary
kable(dim(trn.dat.drp)[2], col.names = c("Variables"), format='html')
kable(corrTabl, digits = 4, align = 'r', format='markdown')
```

#### Hybrid Correlation Matrix

Rather than dropping all of the variables that exceed our threshold, this step extracts their priciple components using PCA. The surviving variables and priciple components are then combined to create our training set. Most of the significant collinearity is removed while maintainining the predictive variance. Depending on your needs, the remaining collinearity may still be too high.

```{r setup_collinearity_hyb, echo=TRUE, eval=TRUE, fig.height=8, fig.width=8, cache=TRUE, collapse=TRUE}
# Reassign variables to the PCA list that push our VIF threshold above 6
var.pck <- setdiff(var.keep, 
                   c("magnet_arm_x", "magnet_forearm_x", "gyros_arm_x", 
                     "magnet_forearm_y", "gyros_forearm_y", "total_accel_dumbbell"))

# Identify the collinear variables
var.pca <- setdiff(colnames(trn.dat.all), c(var.pck, "classe"))

# Preprocess the collinear variables
trn.ppv = preProcess(trn.dat.all[,c(var.pca)], method=c("BoxCox", "center", "scale", "pca"))

# Model the PCA components
trn.pca = (predict(trn.ppv, trn.dat.all[,c(var.pca)]))

# Create hybrid dataframe with noncollinear and pca variables.
trn.dat.hyb  <- cbind(trn.dat.all[,c(var.pck)], trn.pca, classe=trn.dat.all$classe)

# check for any remaining multicollinearity
var.pck <- vif_func(in_frame=trn.dat.hyb[, -dim(trn.dat.hyb)[2]], thresh=9, trace=T)
```

```{r handle_collinearity_hyb, echo=TRUE, eval=TRUE, fig.height=6, fig.width=6, collapse=TRUE}
# Build a correlation matrix on the initial variables, excluding the "classe" variable
corrMatx <- cor(trn.dat.hyb[, -dim(trn.dat.hyb)[2]])
corrSumm <- summary(corrMatx[upper.tri(corrMatx)])[1:6]
corrTabl <- t(as.data.frame(as.vector(corrSumm)))

colnames(corrTabl) <- attr(corrSumm, 'names')
rownames(corrTabl) <- "Hybrid Summary"

# plot the correlation matrix
corrplot(corrMatx, 
         order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.75, tl.col = "black")

# print the correlation summary
kable(dim(trn.dat.hyb)[2], col.names = c("Variables"), format='html')
kable(corrTabl, digits = 4, align = 'r', format='markdown')
```

#### PCA Correlation Matrix

For maximum reduction of multicollinearity, this approach transforms all the variables into princicple components. Notice how we have eliminated all multicollinearity. The only real challenge with this approach, is that it is <u>less intuitive</u> to understand and describe what drives the model performance.

```{r setup_collinearity_pca, echo=TRUE, eval=TRUE, fig.height=8, fig.width=8, cache=TRUE, collapse=TRUE}
# Identify all the variables
var.pca  <- setdiff(colnames(trn.dat.all), c("classe"))

# Preprocess all the variables
trn.ppv = preProcess(trn.dat.all[,c(var.pca)], method=c("BoxCox", "center", "scale", "pca"))

# Model the PCA components
trn.pca = (predict(trn.ppv, trn.dat.all[,c(var.pca)]))

# Create pca dataframe.
trn.dat.pca  <- cbind(trn.pca, classe=trn.dat.all$classe)

# check for any remaining multicollinearity
var.pca <- vif_func(in_frame=trn.dat.pca[, -dim(trn.dat.pca)[2]], thresh=9, trace=T)
```

```{r handle_collinearity_pca, echo=TRUE, eval=TRUE, fig.height=6, fig.width=6, collapse=TRUE}
# Build a correlation matrix on the initial variables, excluding the "classe" variable
corrMatx <- cor(trn.dat.pca[, -dim(trn.dat.pca)[2]])
corrSumm <- summary(corrMatx[upper.tri(corrMatx)])[1:6]
corrTabl <- t(as.data.frame(as.vector(corrSumm)))

colnames(corrTabl) <- attr(corrSumm, 'names')
rownames(corrTabl) <- "PCA Summary"

# plot the correlation matrix
corrplot(corrMatx, 
         order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.75, tl.col = "black")

# print the correlation summary
kable(dim(trn.dat.pca)[2], col.names = c("Variables"), format='html')
kable(corrTabl, digits = 4, align = 'r', format='markdown')
```

## Machine Learning

### Tuning Preparation

We are using a `r dSplit*100`% data split to define our training set. To provide a meaningful estimation of performance, the remaining `r (1-dSplit)*100`% will be divided equally to define our validation and test sets. The individual models are tuned with the training set, and assessed against the validation set. The test set is for estimating the ensemble model performance.

With the [Caret R package][19], we use [repeated k-fold cross-validation][2] with `r cntFld` folds across `r cntRpt` repetition`r if(cntRpt>1){"s"}` to improve our accuracy estimates. We preprocess the data with "centered and scaled" to better expose the underlying structure and relationships to the predictors. We pre-calculate a vector of seeds for reproducibility across multiple parallel model runs.

```{r model_params, echo=TRUE, eval=TRUE, fig.height=8, fig.width=8, collapse=TRUE}
# Select the PCA dataset for training.
trn.dat <- trn.dat.pca

# Partition the data with training, validation, and test sets.
set.seed(1732)
inTrain <- createDataPartition(y=trn.dat$classe, p=dSplit, list=FALSE)
trn.trn <- trn.dat[ inTrain,]
trn.vld <- trn.dat[-inTrain,]
inValid <- createDataPartition(y=trn.vld$classe, p=0.50, list=FALSE)
trn.tst <- trn.vld[-inValid,]
trn.vld <- trn.vld[ inValid,]

# Setup seeds for running fully reproducible model in parallel mode
ivect = cntRpt*cntFld                                        # quantity of integer vectors
seeds <- vector(mode = "list", length = ivect+1)             # length equals the integer vectors plus one
for(i in 1:ivect) seeds[[i]] <- sample.int(n=1000, cntTun*3) # seeds for tuning attempts (*3 for Radial SVM)
seeds[[length(seeds)]] <- sample.int(1000, 1)                # seed for the final model

# Create cross-validation folds to use for model tuning
myMetr <- "Kappa"     # ROC, Kappa, Accuracy
myFlds <- createMultiFolds(y=trn.trn$classe, k = cntFld, times = cntRpt)
myCtrl <- trainControl(method = "repeatedCV", seeds = seeds, allowParallel = TRUE, 
                       #savePredictions=TRUE, classProbs=TRUE, summaryFunction=multiClassSummary,
                       number = cntFld, repeats = cntRpt)

# Standardize the training subset.
myPrep = c("center", "scale")   # pre-process with center and scaling

# house-cleaning
rm("i", "cntFld", "cntRpt", "inTrain", "corrMatx", "vif_func")
```

### Grid Searching {.tabset .tabset-fade}

There are two ways to tune an algorithm using the Caret R package. The first, allow the system to do it automatically by setting the `tuneLength` parameter to indicate the number of different values to try for each algorithm parameter. This makes a crude guess on what values to try, but can quickly get you in a reasonable range. This is the approach we've taken on our first tuning pass. The second approach involves manually setting the tuning grid to search. Our second tuning pass uses this approach. We start with the best performaning parameters from the first tuning pass and create a more focused tuning grid to further refine our parameters.

We have selected three models to tune: 

1) Support Vector Machine (SVM) with a [Radial Basis Function (RBF) kernel][4], 
2) [RandomForest][5], and 
3) [Stochastic Gradient Boost Machine][7]. 

To improve SVM performance we doubled its grid-search area. Also, since the GBM algorithm is computational expensive, we did signficant [manual tuning][13] to attain good perforamnce. For good measure, we also created an [Ensemble][8] (Stack) model using the three separate algorithms to improve overall performance.

> <u>Technical Notes</u>: Since this process can be very resource intensive, we use [OpenBlas][3], which is an optimized library to replace the standard BLAS library used by R. In addition, we use the "doMC" library to specify the number cores to correspond with the maximum number of R jobs to run in parallel. Depending on your tuning parameters and training set size, this process can easily consume all of your computer memory and may take hours to run.

#### SVM Passes

First-pass Logic:

```{r model_svr_fpass, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# First Model: Radial Support Vector Machine
# First Pass: Tunelength
system.time(
    # Fit model to the training set.
    tune.svr1 <- train(classe ~ ., data = trn.trn, method = "svmRadial",
                      tuneLength = cntTun*2, preProcess = myPrep, 
                      metric = myMetr, trControl = myCtrl)
)
tune.svr1;
```

Second-pass Logic:

```{r model_svr_spass, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# First Model: Radial Support Vector Machine
# Second Pass: Grid search based on the parameter values selected in the first pass
mySig = round(tune.svr1$bestTune$sigma, 3)
myCst = tune.svr1$bestTune$C
myGrd = expand.grid(sigma = c(mySig-.005, mySig, mySig+.005),
                    C = c(myCst-5, myCst-2, myCst, myCst+2, myCst+5))

system.time(
    # Fit model to the training set.
    tune.svr2 <- train(classe ~ ., data = trn.trn, method = "svmRadial",
                       tuneGrid = myGrd, preProcess = myPrep, 
                       metric = myMetr, trControl = myCtrl)
)
tune.svr2;
```

#### Random Forest

First-pass Logic:

```{r model_rft_fpass, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# Second Model: Random Forest Model (Extension to Bagging where Trees represent 
#               Bootstrap samples with Bootstrap nodes)
# First Pass: Tune Length
system.time(
    #Fit a random forest model to the training set. Include Importance and Proximity scores
    tune.rft1 <- train(classe ~ ., data = trn.trn, method="rf",
                       #proximity=TRUE, importance=TRUE,  # produce extra info 
                       tuneLength = cntTun, preProcess = myPrep, 
                       metric = myMetr, trControl = myCtrl)
)
tune.rft1
```

Second-pass Logic:

```{r model_rft_spass, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# Second Model: Random Forest Model (Extension to Bagging where Trees represent 
#               Bootstrap samples with Bootstrap nodes)
# Second Pass: Grid search based on the parameter values selected in the first pass
myMtry = max(2, tune.rft1$bestTune$mtry)
myGrd = expand.grid(mtry = c(myMtry-1, myMtry, myMtry+1))

system.time(
    # Fit model to the training set.
    tune.rft2 <- train(classe ~ ., data = trn.trn, method = "rf",
                       tuneGrid = myGrd, preProcess = myPrep, 
                       metric = myMetr, trControl = myCtrl)
)
tune.rft2;
```

#### Gradient Boost

First-pass Logic:

```{r model_gbm_fpass, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# Third Model: Stochastic Gradient Boost Machine
# First Pass: Tune Length
system.time(
    #Fit a random forest model to the training set. Include Importance and Proximity scores
    tune.gbm1 <- train(classe ~ ., data = trn.trn, method="gbm", verbose=FALSE,
                       tuneLength = cntTun, preProcess = myPrep, 
                       metric = myMetr, trControl = myCtrl)
)
tune.gbm1
```

Second-pass Logic:

```{r model_gbm_spass, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# Third Model: Stochastic Gradient Boost Machine
# Second Pass: Grid search based on the parameter values selected in the first pass
myTrees = tune.gbm1$bestTune$n.trees; myTrees
myDepth = tune.gbm1$bestTune$interaction.depth; myDepth
myMinob = tune.gbm1$bestTune$n.minobsinnode; myMinob

# Lower learning rates are preferred, however require a higher number of trees to
# model all the relationships and would be computationally expensive.
# Note: higher values of the following parameters may lead to overfitting:
# * Depth (5-8) should be chosen based on the number of observations and predictors. 
# * Shrinkage (0.05-0.2) controls the learning rate.
myGrd = expand.grid(n.trees = c(myTrees+1199),
                    interaction.depth = c(myDepth+1),
                    shrinkage = c(0.08),
                    n.minobsinnode = c(15:17))

system.time(
    # Fit model to the training set.
    tune.gbm2 <- train(classe ~ ., data = trn.trn, method="gbm", verbose=FALSE,
                       tuneGrid = myGrd, preProcess = myPrep, 
                       metric = myMetr, trControl = myCtrl)
)
tune.gbm2;
```

### Tuning Model Accuracy {.tabset .tabset-fade}

After tuning these models we can identify the parameters that generate the best performance per model and compare how well the models perform against each other.

#### Summary

```{r model_out, echo=TRUE, eval=TRUE, collapse=TRUE}
mcomp = rbind(getTrainPerf(tune.svr2),
              getTrainPerf(tune.rft2),
              getTrainPerf(tune.gbm2))

kable(mcomp)
```

#### Support Vector

```{r model_plot_svm, echo=TRUE, eval=TRUE, collapse=TRUE, fig.height=4, fig.width=7}
plot(tune.svr1, metric = "Kappa", main = "First Pass")
plot(tune.svr2, metric = "Kappa", main = "Second Pass")
```

#### Random Forest

```{r model_plot_rft, echo=TRUE, eval=TRUE, collapse=TRUE, fig.height=4, fig.width=7}
plot(tune.rft1, metric = "Kappa", main = "First Pass")
plot(tune.rft2, metric = "Kappa", main = "Second Pass")
```

#### Gradient Boost

```{r model_plot_gbm, echo=TRUE, eval=TRUE, collapse=TRUE, fig.height=4, fig.width=7}
plot(tune.gbm1, metric = "Kappa", main = "First Pass")
plot(tune.gbm2, metric = "Kappa", main = "Second Pass")
```

#### Density Plot

```{r model_compare, echo=TRUE, eval=TRUE, collapse=TRUE, fig.height=4, fig.width=9}
library(randomForest)
# Compare Models
resamps <- resamples(list(
             '1.SVM.Radial'     = tune.svr2,
             '2.Random.Forest'  = tune.rft2,
             '3.Gradient.Boost' = tune.gbm2
           ))
summary(resamps)

# Using Kappa as a more conservative measure for accuracy.
densityplot(resamps, metric='Kappa', 
            auto.key=list(space='right', 
                          trellis.par.set(superpose.line = list(lwd = 2.5, lty = 1))
                          )
            )

# Compare the lagged differences between the models.
mdiffs <- diff(resamps)
summary(mdiffs)
```

### Fitting the Models {.tabset .tabset-fade}

Now that we have the tuned parameters we select and fit the final models. This is really fast compared to the tuning steps. 

```{r model_pext, echo=TRUE, eval=TRUE, collapse=TRUE}
# Retrive the training errors for the SVM model
err.svr <- tune.svr2$finalModel@error; err.svr

# Retrieve the OOB error for the Random Forest model based on number of trees used
ntr     <- tune.rft2$finalModel$ntree; ntr
err.rft <- tune.rft2$finalModel$err.rate[ntr,1]; err.rft

# Identify the lowest error rate by number of trees
treerr.df <- as.data.frame(tune.rft2$finalModel$err.rate[,1])
treerr.df$trees <- row(treerr.df)
colnames(treerr.df) <- c("err","trees")

# Reduce the number of trees for the Random Forest based on error rate
ntr = as.vector(head(treerr.df[with(treerr.df, order(err)), ], 1)$trees); ntr
err.rft <- tune.rft2$finalModel$err.rate[ntr, 1]; err.rft
```

#### Support Vector

```{r fit_svr, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# Calculate the final model with the tuning parameters from the above.
mySig = tune.svr2$bestTune$sigma
myCst = tune.svr2$bestTune$C
newGrid = expand.grid(sigma=c(mySig), C=c(myCst))

mySig; myCst

system.time(
    # Fit model to the training set.
    fit.svr <- train(classe ~ ., data = trn.trn, method = "svmRadial", 
                     tuneGrid = newGrid, 
                     preProcess = myPrep, trControl = myCtrl)
)
```

#### Random Forest

```{r fit_rft, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# Calculate the final model with the tuning parameters from the above.
ntr

newGrid = expand.grid(mtry=c(tune.rft2$bestTune$mtry))
system.time(
    fit.rft <- train(classe ~ ., data = trn.trn, method="rf", 
                     tuneGrid = newGrid, ntree = ntr,
                     preProcess = myPrep, trControl = myCtrl)
)
```

#### Gradient Boost

```{r fit_gbm, echo=TRUE, eval=TRUE, cache=TRUE, collapse=TRUE}
# Calculate the final model with the tuning parameters from the above.
myTrees = tune.gbm2$bestTune$n.trees
myDepth = tune.gbm2$bestTune$interaction.depth
myShrnk = tune.gbm2$bestTune$shrinkage
myMNode = tune.gbm2$bestTune$n.minobsinnode

myTrees; myDepth; myShrnk; myMNode

myGrd = expand.grid(n.trees = c(myTrees),
                    interaction.depth = c(myDepth),
                    shrinkage = c(myShrnk),
                    n.minobsinnode = c(myMNode))

system.time(
    fit.gbm <- train(classe ~ ., data = trn.trn, method="gbm", verbose=FALSE,
                     tuneGrid = myGrd, preProcess = myPrep, 
                     metric = myMetr, trControl = myCtrl)
)
```

### Model Analysis and Validation {.tabset .tabset-fade}

During this process we run the tuned models against our validation set to assess performance. This model performance indicates that our out-of-sample error is very good and that we have not overfit the model.

```{r model_analysis, echo=TRUE, eval=TRUE, collapse=TRUE}
# Assess the separate model accuracy on the validation set.
yhat.svr <- predict(fit.svr, trn.vld)
yhat.rft <- predict(fit.rft, trn.vld)
yhat.gbm <- predict(fit.gbm, trn.vld)

cm.svr <- confusionMatrix(yhat.svr, trn.vld$classe)
cm.rft <- confusionMatrix(yhat.rft, trn.vld$classe)
cm.gbm <- confusionMatrix(yhat.gbm, trn.vld$classe)
```

#### Support Vector

```{r eval_svr, echo=TRUE, eval=TRUE, collapse=TRUE}
# Confusion Matrix
kable(cm.svr$table, caption = "Confusion Matrix:")
# Class Summary
kable(t(cm.svr$byClass), caption = "Statistics by Class:")
```

#### Random Forest

```{r eval_rft, echo=TRUE, eval=TRUE, collapse=TRUE}
# Confusion Matrix
kable(cm.rft$table, caption = "Confusion Matrix:")
# Class Summary
kable(t(cm.rft$byClass), caption = "Statistics by Class:")
```

#### Gradient Boost

```{r eval_gbm, echo=TRUE, eval=TRUE, collapse=TRUE}
# Confusion Matrix
kable(cm.gbm$table, caption = "Confusion Matrix:")
# Class Summary
kable(t(cm.gbm$byClass), caption = "Statistics by Class:")
```

### Ensemble (Stack) Model {.tabset .tabset-fade}

Although the separate models perform well individually, we built this ensemble model to hightlight the process, and the potential benefit to predictive performance. Different algorithms will find some variables more predictve than others, and will rank them differently. This shows up in the variables of importances (see below). To get additional lift, it is important that the models being stacked have varying variables of importance, because the ensemble model will capitalize on those differences to boost predictive performance. 

After fitting the separate models to our validation set, we create "modelled predictors" so that we can train the ensemble model. Then we assess the model performance against our test set. This "stacked" model should have improved predictive performance over the separate models.

```{r model_stack, echo=TRUE, eval=TRUE, collapse=TRUE}
set.seed(1732)
# Dataframe with predictions of the separate models, combined with the classe variable (from the validation set)
# in preparation for training the ensemble model against the "validation" data.
pred.vld <- data.frame(yhat.svr, yhat.rft, yhat.gbm, classe=trn.vld$classe)

# Train the ensemble model on the "stacked" predictors
# Note: since it was the fastest of the trio, the SVM algorithm used to build the ensemble
fit.stack <- train(classe ~ ., data=pred.vld, method="svmRadial",
                   metric = myMetr, trControl = myCtrl)

# Compare the ensemble model training performance to the separate models.
rbind(getTrainPerf(fit.stack),
      getTrainPerf(fit.svr),
      getTrainPerf(fit.rft),
      getTrainPerf(fit.gbm))

# To test the ensemble performance we create "stacked" predictors based
# on the test set for ensemble prediction assessment.
yhat.svr <- predict(fit.svr, trn.tst)
yhat.rft <- predict(fit.rft, trn.tst)
yhat.gbm <- predict(fit.gbm, trn.tst)

# Create "stacked" dataframe of modelled predicters and the classe variable (from the test set)
# in preparation for training the ensemble model against the "test" data.
pred.tst <- data.frame(yhat.svr, yhat.rft, yhat.gbm, classe=trn.tst$classe)

# Predict the model classe variable using the "stacked" predictors
yhat.stk <- predict(fit.stack, pred.tst)

# Compare the predicted class against the actual class variable.
cm.stk <- confusionMatrix(yhat.stk, trn.tst$classe)
str(cm.stk$byClass)
```

#### Variables of Importance

Below are the top 10 GBM variables in comparison to the other models (100=most important; 0=least important). Notice how the variables of importance vary across models.

```{r variable_import, echo=TRUE, eval=TRUE, collapse=TRUE}
# Compare the variables of importance.
viv = cbind(varImp(tune.gbm2, scale = TRUE)$importance,
            varImp(tune.rft2, scale = TRUE)$importance,
            varImp(tune.svr2, scale = TRUE)$importance)
colnames(viv)[1] = "Gradient_Boost"
colnames(viv)[2] = "Random_Forest"
colnames(viv)[3:7] = c("SVM-A","SVM-B","SVM-C","SVM-D","SVM-E")

kable(head(viv[with(viv, order(-Gradient_Boost)), ], 10),
      digits = 2)
```

#### Model Correlation

Similar to searching for multicolinearity in the predictors of the separate models, we need to check if modelled predictors are fairly uncorrelated. Ideally, the modelled predictors should have less than 75% correlation.

```{r finale_mcoor, echo=TRUE, eval=TRUE, collapse=TRUE, fig.height=4, fig.width=9}
results <- resamples(list(
             SVM.Radial     = fit.svr,
             Random.Forest  = fit.rft,
             Gradient.Boost = fit.gbm
           ))

# check if model predicotrs are fairly un-correlated (< 0.75)
kable(modelCor(results), caption = "Model Correlation Matrix:")
```

#### Density Plots

Although the Gradient Boost and Random Forest models do not perform as well as the Support Vector Machine model, the ensemble algorithm is able to learn from their variance to perform better than all three. 

```{r finale_dplot, echo=TRUE, eval=TRUE, collapse=TRUE, fig.height=5, fig.width=9}
results <- resamples(list(
             '1.SVM.Radial'     = fit.svr,
             '2.Random.Forest'  = fit.rft,
             '3.Gradient.Boost' = fit.gbm,
             '4.Ensemble.Stack' = fit.stack
           ))

# Summarize the ensemble results
summary(results)
lapply(fit.stack, summary)

# Using Kappa as a more conservative measure for accuracy.
densityplot(results, metric='Kappa', 
            auto.key=list(space='right', 
                          trellis.par.set(superpose.line = list(lwd = 2.5, lty = 1))
                          )
            )
```

#### Box Plots

```{r finale_bplot, echo=TRUE, eval=TRUE, collapse=TRUE, fig.height=4, fig.width=8}
# box and whisker plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)
#bwplot(results, metric="ROC")
```

#### Per-Class Statistics

```{r eval_stk, echo=TRUE, eval=TRUE, collapse=TRUE}
# Confusion Matrix
kable(cm.stk$table, caption = "Confusion Matrix:")
# Class Summary
kable(t(cm.stk$byClass), caption = "Statistics by Class:")
```

## Conclusion

Our statistics, including accuracy, [precision, and recall][22], indicate that the ensemble model is well generalized and not overfit to the training data. The ensemble model's estimated training accuracy (`r round(getTrainPerf(fit.stack)$TrainAccuracy * 100, 2)`%) shows a `r round((getTrainPerf(fit.stack)$TrainAccuracy - getTrainPerf(fit.svr)$TrainAccuracy) * 100, 2)`% predictive improvement over the SVM model (`r round(getTrainPerf(fit.svr)$TrainAccuracy * 100, 2)`%). 

Let's see how the ensemble performs compared to the separate models against the test data:

* <u>Ensemble</u>: `r round(confusionMatrix(yhat.stk, trn.tst$classe)[[3]]["Accuracy"] * 100, 2)`%
* <u>SVM-Radial</u>: `r round(confusionMatrix(yhat.svr, trn.tst$classe)[[3]]["Accuracy"] * 100, 2)`%
* <u>Random Forest</u>: `r round(confusionMatrix(yhat.rft, trn.tst$classe)[[3]]["Accuracy"] * 100, 2)`%
* <u>Gradient Boost</u>: `r round(confusionMatrix(yhat.gbm, trn.tst$classe)[[3]]["Accuracy"] * 100, 2)`%

In addition to the improved estimated mean performance, the ensemble model has estimated per-class performance that is equal-to or better than the separate models:

```{r stats_stk, echo=TRUE, eval=TRUE, collapse=TRUE}
# Assess how the ensemble gets higher accuracy
# Compare combined predictor accuracy to the producer’s accuracy (aka sensitivity or recall) for the classes
pred.perf <- rbind(confusionMatrix(yhat.svr, trn.tst$classe)$byClass[, 7],
                   confusionMatrix(yhat.rft, trn.tst$classe)$byClass[, 7],
                   confusionMatrix(yhat.gbm, trn.tst$classe)$byClass[, 7],
                   confusionMatrix(yhat.stk, trn.tst$classe)$byClass[, 7])
row.names(pred.perf) <- c("SVM.Radial", "Random.Forest", "Gradient.Boost", "Ensemble.Stack")
# Class Summary
kable(pred.perf, caption = "Per-class Weigthed Average of Precision and Recall (F-1):")
```

> <u>Technical Notes</u>: ROC (Receiver Operating Characteristic) curve analysis was not designed for multi-classification problems, however using the [pROC][24] library, we calculate the mean area under the curve (**AUC**) on the test data of **`r round(multiclass.roc(as.ordered(trn.tst$classe), as.ordered(yhat.stk))$auc * 100, 2)`%**, which cannot be plotted. To learn more about ROC curve analysis, have a look at this BioMed Central (BMC) research paper. "Xavier Robin, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez and Markus Müller (2011). [pROC: an open-source package for R and S+ to analyze and compare ROC curves][23]. BMC Bioinformatics, 12, p. 77. DOI: 10.1186/1471-2105-12-77".

```{r model_predict, echo=FALSE, eval=FALSE, collapse=TRUE}
#After building the final model and testing it on our validation and test sets, we apply it to the final test file to confirm our prediction accuracy. Our model had a 100% match prediction on the test file. The final output will be applied to our Coursera project submission for grading.

# Final assessment of class file.
# Load the test file
trn.fin = read.csv(tstFile, na.strings=c("NA","NaN", "", " "), stringsAsFactors=FALSE)

# Generate the stack values
yhat.svr <- predict(fit.svr, trn.fin)
yhat.rft <- predict(fit.rft, trn.fin)
yhat.gbm <- predict(fit.gbm, trn.fin)

# Create the stacked dataframe
pred.fin <- data.frame(yhat.svr, yhat.rft, yhat.gbm)

# Predict the ensemble model results
fin.pred <- predict(fit.stack, pred.fin); fin.pred
```

```{r model_conclude, echo=FALSE, eval=FALSE, collapse=TRUE}
# Output Results
library(stringr)
for(x in 1:length(fin.pred)) {
    filename = paste0("data/problem_id_", str_pad(x, 2, pad = "0" ), ".txt")
    write.table(fin.pred[x], file=filename, 
                quote=FALSE, row.names=FALSE, col.names=FALSE)
}
```

[1]: http://groupware.les.inf.puc-rio.br/har#ixzz3de67BWZU
[2]: http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm 
[3]: http://www.openblas.net/
[4]: https://en.wikipedia.org/wiki/Radial_basis_function_kernel
[5]: https://en.wikipedia.org/wiki/Random_forest
[6]: https://beckmw.wordpress.com/2013/02/05/collinearity-and-stepwise-vif-selection/
[7]: https://en.wikipedia.org/wiki/Gradient_boosting
[8]: https://en.wikipedia.org/wiki/Ensemble_learning
[9]: https://en.wikipedia.org/wiki/Partial_least_squares_regression
[10]: https://en.wikipedia.org/wiki/Principal_component_analysis
[11]: http://blog.minitab.com/blog/applying-statistics-in-quality-projects/how-could-you-benefit-from-a-box-cox-transformation
[12]: https://en.wikipedia.org/wiki/Variance_inflation_factor
[13]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/
[14]: https://www.rstudio.com/resources/cheatsheets/
[15]: https://support.rstudio.com/hc/en-us/categories/200035113-Documentation
[16]: https://en.wikipedia.org/wiki/Reproducibility
[17]: https://github.com/roobyz/PredictiveML
[18]: http://rmarkdown.rstudio.com/lesson-1.html
[19]: https://topepo.github.io/caret/
[20]: http://rmarkdown.rstudio.com/
[21]: https://rstudio.com/
[22]: https://en.wikipedia.org/wiki/Precision_and_recall
[23]: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-77
[24]: http://web.expasy.org/pROC/
